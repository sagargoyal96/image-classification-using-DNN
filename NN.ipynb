{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import pickle\n",
    "from enum import Enum\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Objects(Enum):\n",
    "    banana=1\n",
    "    bulldozer=2\n",
    "    chair=3\n",
    "    eyeglasses=4\n",
    "    flashlight=5\n",
    "    foot=6\n",
    "    hand=7\n",
    "    harp=8\n",
    "    hat=9\n",
    "    keyboard=10\n",
    "    laptop=11\n",
    "    nose=12\n",
    "    parrot=13\n",
    "    penguin=14\n",
    "    pig=15\n",
    "    skyscraper=16\n",
    "    snowman=17\n",
    "    spider=18\n",
    "    trombone=19\n",
    "    violin=20\n",
    "\n",
    "def create_train():\n",
    "    lengths=[]\n",
    "    filename=\"train/\"+Objects(1).name+ \".npy\"\n",
    "    obj_np=np.load(filename)\n",
    "    obj_np=np.array(obj_np/255)\n",
    "    lengths.append(len(obj_np))\n",
    "    for i in range(1,20):\n",
    "        filename=\"train/\"+Objects(i+1).name+ \".npy\"\n",
    "        temp=np.load(filename)\n",
    "        temp=np.array(temp/255)\n",
    "        lengths.append(len(temp))\n",
    "        obj_np=np.vstack((obj_np,temp))\n",
    "\n",
    "\n",
    "#     print(lengths)\n",
    "    return obj_np, lengths\n",
    "        \n",
    "X_train, lengths=create_train()\n",
    "X_train= StandardScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "def create_test():\n",
    "    filename=\"test/test.npy\"\n",
    "    obj_np=np.load(filename)\n",
    "    return obj_np\n",
    "\n",
    "#     print(lengths)\n",
    "X_test=create_test()\n",
    "X_test= StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# print(X_train)\n",
    "def csv_writer(test_answers):\n",
    "    with open('answers.csv', 'w') as csvfile:\n",
    "        mywriter = csv.writer(csvfile)\n",
    "        mywriter.writerow([\"ID\", \"CATEGORY\"])\n",
    "        for i in range(len(test_answers)):\n",
    "            mywriter.writerow([i, Objects(test_answers[i]+1).name])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def createYtrain():\n",
    "    Y_train=[]\n",
    "    for i in range(20):\n",
    "        for j in range(lengths[i]):\n",
    "            temp=np.zeros((1,20))\n",
    "            temp[0][i]+=1\n",
    "            Y_train.append(temp[0])\n",
    "    return np.array(Y_train)\n",
    "\n",
    "def createY():\n",
    "    Y_train=[]\n",
    "    for i in range(20):\n",
    "        for j in range(lengths[i]):\n",
    "            Y_train.append(i)\n",
    "    return np.array(Y_train)\n",
    "\n",
    "Y_train=createYtrain()\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# max_acc=0\n",
    "# for i in range(1):\n",
    "#     svm_trainX, svm_testX, svm_trainY, svm_testY = train_test_split(newX_train, Y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    trainX, testX, trainY, testY = train_test_split(X_train, Y_train, test_size=0.1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "# np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(2000, input_dim=784, activation='sigmoid'))\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 39s 436us/step - loss: 1.2325 - acc: 0.6384\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 38s 422us/step - loss: 0.8964 - acc: 0.7351\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 34s 377us/step - loss: 0.6981 - acc: 0.7947\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 34s 379us/step - loss: 0.5219 - acc: 0.8460\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 41s 459us/step - loss: 0.3542 - acc: 0.8982\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 34s 377us/step - loss: 0.2079 - acc: 0.9473\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 35s 385us/step - loss: 0.1025 - acc: 0.9827\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 33s 372us/step - loss: 0.0460 - acc: 0.9962\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 34s 381us/step - loss: 0.0202 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 36s 399us/step - loss: 0.0099 - acc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14835e668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(testX[0]))\n",
    "# huhu=np.matrix(testX[0].reshape((784,)))\n",
    "# print(huhu)\n",
    "myoutput=model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0144550e-05 6.2813458e-04 1.6114591e-06 3.9832923e-07 1.0240635e-04\n",
      " 4.4092169e-05 1.9608638e-08 5.0149436e-05 1.2586495e-03 1.2022912e-03\n",
      " 4.6949833e-07 9.5369323e-05 4.7128223e-04 4.9721883e-05 8.2135712e-06\n",
      " 1.9707892e-04 9.9542505e-01 2.9612117e-06 3.9036179e-04 1.1458499e-05]\n"
     ]
    }
   ],
   "source": [
    "# print((myoutput[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "labels=np.argmax(myoutput, axis=1)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  3 14 ...  3  6 16]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10 14 ...  1 15 10]\n"
     ]
    }
   ],
   "source": [
    "correct_labels=np.argmax(testY, axis=1)\n",
    "print(correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 19s 194us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09125252511095255, 0.97974]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_outp=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# print(fin_outp)\n",
    "labels=np.argmax(fin_outp, axis=1)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  8 13 ...  2  5 15]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_writer(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import h5py\n",
    "model.save('nn_2000_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
